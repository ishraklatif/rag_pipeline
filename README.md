RAG â€“ Lightweight Retrieval-Augmented Generation Pipeline
This is a modular, local-first RAG pipeline built with LangChain 1.x, Hugging Face models, and ChromaDB.
It retrieves knowledge from local documents or web pages, embeds them with bge-base-en-v1.5,
and generates context-aware answers using a foundation model such as Phi-3-mini-4k-instruct.
ğŸš€ Features
ğŸ” Dual ingestion: web (via URL) or local text files.
ğŸ§© Embeddings powered by BAAI/bge-base-en-v1.5.
ğŸ§  Generation with microsoft/Phi-3-mini-4k-instruct (MPS-compatible on Mac).
ğŸ’¾ Persistent vector storage using ChromaDB.
âš™ï¸ Fully local â€” no external APIs required.
ğŸ§± Simple CLI interface to query your documents.
ğŸ§© Project Structure
rag_pipeline/
â”‚
â”œâ”€â”€ config.py              # All model and path settings
â”œâ”€â”€ ingest.py              # Loads & chunks local/web documents
â”œâ”€â”€ embeddings_store.py    # Builds or loads Chroma vector store
â”œâ”€â”€ llm_zephyr.py          # Loads foundation LLM (reads from config)
â”œâ”€â”€ rag_pipeline.py        # LCEL-based RAG chain
â”œâ”€â”€ run_rag.py             # CLI entry point
â””â”€â”€ .gitignore
âš™ï¸ Setup
# 1. Create a virtual environment
python3 -m venv venv
source venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt
â–¶ï¸ Run the Pipeline
python run_rag.py
Then choose your data source:
[1] ğŸŒ Web (https://ishraklatif.github.io)
[2] ğŸ“ Local documents (/data folder)
[3] ğŸ”€ Both sources
Once the RAG chain builds, you can interact via CLI:
ğŸ§  Question: Who is Ishrak?
ğŸ¤– Answer: [Model response...]
ğŸ§  Configuration
Edit config.py to switch models or adjust parameters:
LLM_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"   # Foundation model
EMBEDDING_MODEL_NAME = "BAAI/bge-base-en-v1.5"        # Encoder
HF_DEVICE = "mps"                                     # "mps" for Mac, "cuda" or "cpu" otherwise
ğŸ“¦ Vectorstore & Data
Documents are stored locally in /data
Embeddings persist in /chroma_db
Both are ignored via .gitignore
ğŸ§¾ License
MIT â€” free to use, modify, and extend.